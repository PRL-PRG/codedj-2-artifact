---
title: "Simple Selection"
output: html_notebook
---

> This is a recreation of the selection & analysis, but this time we do not complicate things and only do what we plan to use:)

```{r}
source("artifact/helpers.R")
source("artifact/latex-log.R")
initialize_log(name = "selection")

N_TOP_STARS = 1000

prettify_value = function(x) {
  if (x == 1000)
    "1K"
  else if (x == 10000)
    "10K"
  else if (x == 100000)
    "100K"
  else if (x == 1000000)
    "1M"
  else if (x == 10000000)
    "10M"
  else if (x == 100000000)
    "100M"
  else if (x == 1000000000)
    "1B"
  else if (x == 10000000000)
    "10B"
  else if (x == 100000000000)
    "100B"
  else
    paste(x)
}

```

Here's the idea revisited: Maybe there is something to stars: Everyone wants developed projects, and they use stars as a proxy for that. This is wrong because even if stars were proxy of the qualities of developed projects, we all agree that not all developerd projects are also popular. The popularity is really one pretty much random subset which everyone keeps repeating. Bad. So let's use stars not to choose the projects, but to determine the properties of developed projects and then do random sample from all project in the dataset that meet this criteria. 

Take the top 1k by stars. Use domain knowledge to determine which attributes of a project are actually directly representative of its level of development. Then use basic statistics - take the lower 5% cutoff for the projects in the stars by the categories selected and use those to filter out the dataset:

These are the columns that we have selected as being representative of the development. They are not necessarily exhaustive, nor might they be the best possible representations of the phenomena they want to describe. But strength is in numbers (i.e. if you sample from entire GH, the absolute precission of these criteria is of debatable usefulness). Also they nicely showcase CodeDJ capabilities.

```{r}
columns = c("max_hindex1", "lifetime", "users", "locs", "snapshots", "commits")
names(columns) = c("C-Index", "Age", "Devs", "Locs", "Versions", "Commits")
```

Let's first calculate the cutoffs for the respective criteria and visualize them:

```{r}
# creates a cummulative table for the dataset and given column
create_cummulative_table = function(data, column, breaks) {
  column = as.symbol(column)
  data = data %>% select(!!column)
  data_rows = nrow(data)
  value = rep(0, length(breaks))
  pct = rep(0, length(breaks))
  for (i in 1:length(breaks)) {
    remaining = nrow(data %>% filter(!!column >= breaks[[i]]))
    value[[i]] = remaining
    pct[[i]] = remaining / data_rows
  }
  data.frame(
    cutoff = breaks,
    members = value,
    pct = pct
  )
}

cutoff_all_f = function(x) { quantile(x, 0.05) }
cutoff_stars_f = function(x) { quantile(x, 0.05) }

# given a column name, create the cummulative graph for it
cummulative_graph = function(dall, dstars, col_name, title = col_name) {
  # get the cummulative tables    
  call = create_cummulative_table(dall, col_name, quantile(dall[[col_name]], 0:100 / 100))
  cstars = create_cummulative_table(dstars, col_name, quantile(dstars[[col_name]], 0:100 / 100))
  # merge them into a single data frame for easier printing
  call$source = rep("Dataset", nrow(call))
  cstars$source = rep("Top Stars", nrow(cstars))
  cboth = rbind(call, cstars)

  #cutoff_all = cutoff_all_f(dall[[col_name]])
  #if (cutoff_all == min(dall[[col_name]])) {
  #  cutoff_all = cutoff_all + 1
  #}
  cutoff_stars = cutoff_stars_f(dstars[[col_name]])
  column = as.symbol(col_name)
  
  dcutoff = dall %>% filter(!!column >= cutoff_stars)
  
  #cutoff_all_pct = nrow(dall %>% filter(!!column >= cutoff_all)) / nrow(dall)
  #cutoff_stars_pct = nrow(dcutoff) / nrow(dall)
  cutoff_all_pct = nrow(dcutoff) / nrow(dall)
  cutoff_stars_pct = nrow(dstars %>% filter(!!column >= cutoff_stars)) / nrow(dstars)
  
  ccutoff = create_cummulative_table(dcutoff, col_name, quantile(dcutoff[[col_name]], 0:100 / 100))
  ccutoff$source = "Developed"
  cboth = rbind(cboth, ccutoff)
  
  v_breaks = sort(c(0, 0.25, 0.5, 0.75, 1, cutoff_all_pct, cutoff_stars_pct))
  v_labels = round(v_breaks * 100, 0)
  
  h_breaks = sort(c(0, 1, 10, 100, 1000, 10000, 100000, 1000000, 10000000, 100000000, 100000000000, cutoff_all, cutoff_stars))
  h_labels = sapply(h_breaks, prettify_value)
  
  plot = ggplot(cboth) +
    geom_line(aes(x = cutoff, y = pct, color = source)) +
    #geom_vline(xintercept = cutoff_all, linetype = "dashed", color = "black") +
    geom_hline(yintercept = cutoff_all_pct, linetype = "dashed", color = "black") +
    geom_vline(xintercept = cutoff_stars, linetype = "dotted", color = "red") +
    geom_hline(yintercept = cutoff_stars_pct, linetype = "dashed", color = "red") +
  #scale_x_log10(breaks = c(0,1,10,100,1000,10000,100000), labels = c("0", "1","10", "100", "1K", "10K", "100K")) +
    scale_x_log10(breaks = h_breaks, labels = h_labels) +
    scale_y_continuous(breaks = v_breaks, labels = v_labels) + 
    scale_color_manual(values = c("black", "gray", "red")) +
    xlab(title) + ylab("%") +
    theme_classic()
  list(
  name = col_name, 
    title = title,
    plot = plot, cutoff_all = cutoff_all, cutoff_stars = cutoff_stars
  )
}

language_plots = function(dataset, top_stars, columns) {
  cdata = list()
  for (name in names(columns)) {
    cdata[[name]] = cummulative_graph(dataset, top_stars, columns[[name]], title = name)
  }
  cdata  
}
```

```{r}
# note the datasets come from the main file
# java
java_dataset = java
java_topstars = java_dataset %>% filter(stars > 0) %>% arrange(desc(stars)) %>% head(N_TOP_STARS)
cjava = language_plots(java_dataset, java_topstars, columns)
#python
python_dataset = python
python_topstars = python_dataset %>% filter(stars > 0) %>% arrange(desc(stars)) %>% head(N_TOP_STARS)
cpython = language_plots(python_dataset, python_topstars, columns)
#js
js_dataset = js
js_topstars = js_dataset %>% filter(stars > 0) %>% arrange(desc(stars)) %>% head(N_TOP_STARS)
cjs = language_plots(js_dataset, js_topstars, columns)

```

### Java

```{r}
cjava[["C-Index"]]$plot
cjava[["Age"]]$plot
cjava[["Devs"]]$plot
cjava[["Locs"]]$plot
cjava[["Versions"]]$plot
cjava[["Commits"]]$plot
```
What these graphs show: The horizontal axis is the value of the attribute and is log10 scale. The vertical axis is the % of the dataset. A point on the graph at [A;B] is interpreted as: A% of the dataset have the attribute's value >= B. We show three datasets: The full dataset (in black), the top 1k projects by stars (red) and finally the developed projects (gray). There are also intersect lines, where the dotted vertical line is the cutoff value for the attribute at 5% of the top stars dataset. I.e. such value that only 5% of the top star projects have the attribute's value lower than it. The black dashed horizontal line is then the percentage of the entire dataset that has the value >= than the cutoff. 

We see that this varies a lot, i.e. the C-Index alone would suggest that 48% of the projects are developed, while the age of the project is the most restrictive, suggesting than only 12% of all the projects can be considered developed. 

```{r}
LOG("javaCIndexCutoff", cjava[["C-Index"]]$cutoff_stars)
LOG("javaAgeCutoff", cjava[["Age"]]$cutoff_stars)
LOG("javaDevsCutoff", cjava[["Devs"]]$cutoff_stars)
LOG("javaLocsCutoff", cjava[["Locs"]]$cutoff_stars)
LOG("javaVersionsCutoff", cjava[["Versions"]]$cutoff_stars)
LOG("javaCommitsCutoff", cjava[["Commits"]]$cutoff_stars)
```


### Python

```{r}
cpython[["C-Index"]]$plot
cpython[["Age"]]$plot
cpython[["Devs"]]$plot
cpython[["Locs"]]$plot
cpython[["Versions"]]$plot
cpython[["Commits"]]$plot
```
```{r}
LOG("pythonCIndexCutoff", cpython[["C-Index"]]$cutoff_stars)
LOG("pythonAgeCutoff", cpython[["Age"]]$cutoff_stars)
LOG("pythonDevsCutoff", cpython[["Devs"]]$cutoff_stars)
LOG("pythonLocsCutoff", cpython[["Locs"]]$cutoff_stars)
LOG("pythonVersionsCutoff", cpython[["Versions"]]$cutoff_stars)
LOG("pythonCommitsCutoff", cpython[["Commits"]]$cutoff_stars)
```
### JavaScript

```{r}
cjs[["C-Index"]]$plot
cjs[["Age"]]$plot
cjs[["Devs"]]$plot
cjs[["Locs"]]$plot
cjs[["Versions"]]$plot
cjs[["Commits"]]$plot
```
```{r}
LOG("jsCIndexCutoff", cjs[["C-Index"]]$cutoff_stars)
LOG("jsAgeCutoff", cjs[["Age"]]$cutoff_stars)
LOG("jsDevsCutoff", cjs[["Devs"]]$cutoff_stars)
LOG("jsLocsCutoff", cjs[["Locs"]]$cutoff_stars)
LOG("jsVersionsCutoff", cjs[["Versions"]]$cutoff_stars)
LOG("jsCommitsCutoff", cjs[["Commits"]]$cutoff_stars)
```

## Aggregating the cutoffs

Now that we have the cutoffs for each of the attributes, we can aggregate them. Let's now look at the aggregation by majority of votes, i.e. how the selection changes if we say that a project must be developed by at least N attribute values for N from 1 to number of attributes:

```{r}
aggregate_cutoffs = function(dataset, cutoffs, lang) {
    x = rep(0, nrow(dataset))
    for (name in names(cutoffs)) {
        co = cutoffs[[name]]
        x = x + (dataset[[co$name]] >= co$cutoff_stars)
    }
    sizes = c()
    pcts = c()
    for (i in 1:length(cutoffs)) {
        n = sum(x >= i)
        sizes = c(sizes, n)
        pcts = c(pcts, n * 100 / nrow(dataset))
    }
    data.frame(
        lang = rep(lang, length(sizes)),
        i = 1:length(sizes),
        n = sizes,
        pct = pcts
    )
}
```

```{r}
ajava = aggregate_cutoffs(java_dataset, cjava, "Java")
apy = aggregate_cutoffs(python_dataset, cpython, "Python")
ajs = aggregate_cutoffs(js_dataset, cjs, "JavaScript")
asels = rbind(ajava, apy, ajs)
```

```{r}
ggplot(asels) +
geom_col(aes(x = i, fill = lang, y = pct), position = position_dodge())
```
The range is quite high, but to compare apples with apples we are going to use the most restrictive selection (otherwise the comparison with top stars would be weird). However this is tunable and explicit so other possibilities are definitely available. 

### Developed Datasets

```{r}
create_developed_dataset = function(dataset, cutoffs, lang, i = length(cutoffs)) {
    x = rep(0, nrow(dataset))
    for (name in names(cutoffs)) {
        co = cutoffs[[name]]
        x = x + (dataset[[co$name]] >= co$cutoff_stars)
    }
    dataset$x=x
    result = dataset %>% filter(x >= i) %>% select(-x)
    LOGPct(paste0("developed", lang), nrow(result), nrow(dataset))
    #cat(paste0(lang, ": ", nrow(result), " (", nrow(result) * 100 / nrow(dataset), "%)\n"))
    result
}
```

```{r}
java_developed = create_developed_dataset(java_dataset, cjava, "Java")
python_developed = create_developed_dataset(python_dataset, cpython, "Python")
js_developed = create_developed_dataset(js_dataset, cjs, "JavaScript")
```

Allright. Now let's plot their histograms so that we can judge them against the top stars and the whole dataset:

```{r}
get_selection_histogram = function(dataset, stars, developed, columns, title) {
  dataset = dataset %>% select(c("id", columns))
  stars = stars %>% select(c("id", columns))
  developed = developed %>% select(c("id", columns))
  # calculate the medians to be displayed too
  medians = data.frame(
    valueDataset = sapply(names(columns), function(x) {median(dataset[[x]])}),
    valueStars = sapply(names(columns), function(x) {median(stars[[x]])}),
    valueDeveloped = sapply(names(columns), function(x) {median(developed[[x]])}),
    variable = factor(names(columns), levels = rev(names(columns)))
  )
  # melt for the histograms
  dataset = melt(dataset, "id")
  stars = melt(stars, "id")
  developed = melt(developed, "id")
  stars$kind="Stars"
  dataset$kind = "All"
  developed$kind = "Developed"
  all = rbind(dataset, stars, developed)
  all$kind = factor(all$kind, levels = c("All", "Stars", "Developed"))
  all$variable = factor(all$variable, levels = rev(names(columns)))
  ggplot() +
    geom_density_ridges(data = all, aes(x = value, y = variable, color = kind, height = ..ndensity.., scale = 0.7), stat = "binline", bins = 50, draw_baseline = FALSE, alpha = 0.5, fill = NA) +
    geom_segment(data = medians, aes(x = valueDataset, xend = valueDataset, y = as.numeric(variable)-0.1, yend = as.numeric(variable) + 0.8), color = "black") +
    geom_segment(data = medians, aes(x = valueStars, xend = valueStars, y = as.numeric(variable)-0.1, yend = as.numeric(variable) + 0.8), color = "red") +
    geom_segment(data = medians, aes(x = valueDeveloped, xend = valueDeveloped, y = as.numeric(variable)-0.1, yend = as.numeric(variable) + 0.8), color = "blue") +
    theme_ridges() + 
    theme(legend.position = "bottom") +
    ylab("") + xlab("") +
    scale_x_continuous(breaks = c(1, 10, 100, 1e3, 1e4, 1e5, 1e6, 1e7, 1e8), labels = c("1","10", "100", "1k", "10k", "100k", "1m", "10m", "100m"), trans = "pseudo_log", limits=c(0, 10000000)) +
    scale_y_discrete(expand = expand_scale(mult = c(0.01, 0.25))) +
    scale_color_manual(values = c("black", "red","blue"), name = "") +
    theme(legend.position = c(0.6, 0.93), legend.direction = "vertical", plot.margin = margin(t = 0, b = 0, l =0, r = 0)) +
    ggtitle(title)
}
```

```{r}
get_selection_histogram(java_dataset, java_topstars, java_developed, columns, "Java")
get_selection_histogram(python_dataset, python_topstars, python_developed, columns, "Python")
get_selection_histogram(js_dataset, js_topstars, js_developed, columns, "JavaScript")
```

# Subsets

> The subsets are generated by CodeDJ actually using the cutoffs calculated above. Arguably CodeDJ can be updated to do the cutoffs itself. 

# Missing Projects Analysis

> TODO do we want this? 













