---
title: "Project Selection Analysis"
output: html_notebook
---

```{r}
source("artifact/helpers.R")
source("artifact/latex-log.R")
initialize_log(name = "selection")

N_TOP_STARS = 1000
```

Here is the idea: The dataset is large and we need statistically sound ways to determine how to remove the uninteresting projects from it. As an example we can say that whether projects are developed or not depends on the number of commits they have. But where to draw the line and say projects with less than N commits are not considered developed. 

The obvious simple solution is to use the widely accepted 5% cutoff. We can illustrate the bad representativeness of the top stars projects by using the 5% cutoff calculated on the top stars projects, but on the entire population. 

> Finally, instead of 5% we can use the manual analysis of top starred projects to give us different number. However we have currently found this number to be at least 2 to 5% (it can be more, but due to the way we did the manual classification, these were the numbers of projects we saw). We can either ignore this, or we can redo the manual analysis. For now this document ignores this option.

# Cummulative graphs for various attributes

First, we plot cummulative graphs for the various attributes CodeDJ offers:

```{r}
# creates a cummulative table for the dataset and given column
create_cummulative_table = function(data, column, breaks) {
  column = as.symbol(column)
  data = data %>% select(!!column)
  data_rows = nrow(data)
  value = rep(0, length(breaks))
  pct = rep(0, length(breaks))
  for (i in 1:length(breaks)) {
    remaining = nrow(data %>% filter(!!column >= breaks[[i]]))
    value[[i]] = remaining
    pct[[i]] = remaining / data_rows
  }
  data.frame(
    cutoff = breaks,
    members = value,
    pct = pct
  )
}

prettify_value = function(x) {
  if (x == 1000)
    "1K"
  else if (x == 10000)
    "10K"
  else if (x == 100000)
    "100K"
  else if (x == 1000000)
    "1M"
  else if (x == 10000000)
    "10M"
  else if (x == 100000000)
    "100M"
  else if (x == 1000000000)
    "1B"
  else if (x == 10000000000)
    "10B"
  else if (x == 100000000000)
    "100B"
  else
    paste(x)
}

# given a column name, create the cummulative graph for it
cummulative_graph = function(dall, dstars, col_name, cutoff = 0.05, title = col_name) {
  # get the cummulative tables    
  call = create_cummulative_table(dall, col_name, quantile(dall[[col_name]], 0:100 / 100))
  cstars = create_cummulative_table(dstars, col_name, quantile(dstars[[col_name]], 0:100 / 100))
  # merge them into a single data frame for easier printing
  call$source = rep("Dataset", nrow(call))
  cstars$source = rep("Top Stars", nrow(cstars))
  cboth = rbind(call, cstars)

cutoff_all = quantile(dall[[col_name]], cutoff)
  if (cutoff_all == min(dall[[col_name]])) {
    cutoff_all = cutoff_all + 1
  }
  cutoff_stars = quantile(dstars[[col_name]], cutoff)
  column = as.symbol(col_name)
  
  dcutoff = dall %>% filter(!!column >= cutoff_stars)
  
  cutoff_all_pct = nrow(dall %>% filter(!!column >= cutoff_all)) / nrow(dall)
  cutoff_stars_pct = nrow(dcutoff) / nrow(dall)
  
  ccutoff = create_cummulative_table(dcutoff, col_name, quantile(dcutoff[[col_name]], 0:100 / 100))
  ccutoff$source = "Remainder"
  cboth = rbind(cboth, ccutoff)
  
  v_breaks = sort(c(0, 0.25, 0.5, 0.75, 1, cutoff_all_pct, cutoff_stars_pct))
  v_labels = round(v_breaks * 100, 0)
  
  h_breaks = sort(c(0, 1, 10, 100, 1000, 10000, 100000, 1000000, 10000000, 100000000, 100000000000, cutoff_all, cutoff_stars))
  h_labels = sapply(h_breaks, prettify_value)
  
  plot = ggplot(cboth) +
    geom_line(aes(x = cutoff, y = pct, color = source)) +
    geom_vline(xintercept = cutoff_all, linetype = "dashed", color = "black") +
    geom_hline(yintercept = cutoff_all_pct, linetype = "dashed", color = "black") +
    geom_vline(xintercept = cutoff_stars, linetype = "dashed", color = "red") +
    geom_hline(yintercept = cutoff_stars_pct, linetype = "dashed", color = "red") +
    #scale_x_log10(breaks = c(0,1,10,100,1000,10000,100000), labels = c("0", "1","10", "100", "1K", "10K", "100K")) +
    scale_x_log10(breaks = h_breaks, labels = h_labels) +
    scale_y_continuous(breaks = v_breaks, labels = v_labels) + 
    scale_color_manual(values = c("black", "gray", "red")) +
    xlab(title) + ylab("%") +
    theme_classic()
  list(
  name = col_name, 
    title = title,
    plot = plot, cutoff_all = cutoff_all, cutoff_stars = cutoff_stars
  )
}
```

Let's do the graphs for the attributes we are looking into now for all three languages:

```{r}
columns = c("max_hindex1", "lifetime", "users", "locs", "snapshots", "commits")
names(columns) = c("C-Index", "Age", "Devs", "Locs", "Versions", "Commits")

language_plots = function(dataset, top_stars, columns) {
  cdata = list()
  for (name in names(columns)) {
    cdata[[name]] = cummulative_graph(dataset, top_stars, columns[[name]], title = name)
  }
  cdata  
}
```

```{r}
# java
java_dataset = java
java_topstars = java_dataset %>% filter(stars > 0) %>% arrange(desc(stars)) %>% head(N_TOP_STARS)
cjava = language_plots(java_dataset, java_topstars, columns)
#python
python_dataset = python
python_topstars = python_dataset %>% filter(stars > 0) %>% arrange(desc(stars)) %>% head(N_TOP_STARS)
cpython = language_plots(python_dataset, python_topstars, columns)
#js
js_dataset = js
js_topstars = js_dataset %>% filter(stars > 0) %>% arrange(desc(stars)) %>% head(N_TOP_STARS)
cjs = language_plots(js_dataset, js_topstars, columns)
```
### Java

```{r}
cjava[["C-Index"]]$plot
cjava[["Age"]]$plot
cjava[["Devs"]]$plot
cjava[["Locs"]]$plot
cjava[["Versions"]]$plot
cjava[["Commits"]]$plot
```

### Python

```{r}
cpython[["C-Index"]]$plot
cpython[["Age"]]$plot
cpython[["Devs"]]$plot
cpython[["Locs"]]$plot
cpython[["Versions"]]$plot
cpython[["Commits"]]$plot
```

### JavaScript

```{r}
cjs[["C-Index"]]$plot
cjs[["Age"]]$plot
cjs[["Devs"]]$plot
cjs[["Locs"]]$plot
cjs[["Versions"]]$plot
cjs[["Commits"]]$plot
```

# Datasets for analysis

For each language, we prepare the following three datasets:

- the top 1k stars, without the projects below the cutoff (i.e. ~5% of the top stars) (the red line after the *red* cutoff above)
- the full dataset without projects below the top stars 5% cutoff (the black line after the *red* cutoff above)
- the full dataset without lowest 5% projects (the black line after the black cutoff above)

These should roughly correspond to the top stars w/o misfits, the definitely interesting projects and the definitely uninteresting projects removed. 

```{r}
create_datasets = function(dataset, top_stars, cutoffs) {
  stars = top_stars
  good = dataset
  not_bad = dataset
  for (x in cutoffs) {
    colname = as.symbol(x$name)
    stars = stars %>% filter(!!colname >= x$cutoff_stars)
    good = good %>% filter(!!colname >= x$cutoff_stars)
    not_bad = not_bad %>% filter(!!colname >= x$cutoff_all)
  }
  cat(paste("top_stars:", nrow(stars), " (", nrow(stars) * 100 / nrow(top_stars), "%)\n"))
  cat(paste("good:", nrow(good), " (", nrow(good) * 100 / nrow(dataset), "%)\n"))
  cat(paste("not_bad:", nrow(good), " (", nrow(not_bad) * 100 / nrow(dataset), "%)\n"))
  list(stars = stars, good = good, not_bad = not_bad)
}
```

```{r}
cat("Java:\n");
java_datasets = create_datasets(java_dataset, java_topstars, cjava)
cat("Python:\n");
python_datasets = create_datasets(python_dataset, python_topstars, cjava)
cat("JS:\n");
js_datasets = create_datasets(js_dataset, js_topstars, cjava)
```




Allright. So maybe let's focus on developed projects because that is what the paper does. In the paper we have empirically set developed projects to be >= 100 LOC, >= 10 commits and >= 7 days of lifetime. But this is arbitrary. Now we have the other selection as well:

```{r}
augment_with_cutoffs = function(dataset, cutoffs) {
  sum = rep(0, nrow(dataset))
  for (coff in cutoffs) {
    x = as.numeric(dataset[[coff$name]] >= coff$cutoff_stars)
    sum = sum + x
    dataset[[paste("cutoff_", coff$name)]] = x
  }
  dataset$cutoff_sum = sum
  dataset
}
dataset = augment_with_cutoffs(dataset, cdata)
```

```{r}
nrow(dataset %>% filter(locs >= 100 & commits >= 10 & lifetime >= 7))
nrow(dataset %>% filter(cutoff_sum >= 1))
nrow(dataset %>% filter(cutoff_sum >= 2))
nrow(dataset %>% filter(cutoff_sum >= 3))
nrow(dataset %>% filter(cutoff_sum >= 4))
nrow(dataset %>% filter(cutoff_sum >= 5))
nrow(dataset %>% filter(cutoff_sum >= 6))
```

Nice. So especially the last one might be interesting as the number of projects in there is actually comparable with the number of starred projects we started with... 

Talking to konrad, the plan now is as follows: Instead of project selections for various criteria (developed, popular, large, whatnot) stick with developed, because the paper as is is already centered around them: Everyone wants developed projects. Stars are bad proxy for their selection, but the interesting idea is that maybe they are a good proxy for finding them. We now present various statistically sound ways to select developed projects from a larger corpus. While we claim those selections are all valid, they are by no means exhaustive and we recommend that our readers follow those guidelines, but apply their own domain knowledge to their papers:

- statistically trivial cutoff of the lowest 5% in the attributes that are deemed relevant to whether a project is developed or not
- domain knowledge such as projects age >= a week, >= 10 commits and >= 100 LOC gets rid of started, but never really developed tombstones, or projects whose age is greater than 6 mo gets rid of students projects, etc. We can do multiple of those. 
- or we can say stars are good proxy for the properties, but bad proxy for the sampling. Take top N by stars, get their attributes (5% cutoff) and then select projects matching that criteria from the entire dataset (the graphs above)

After we get those subsets, we need to find a decent way to analyze them. This is TBD stil. 
































































```{r}
col_name = "commits"
dall = dataset
dstars = top_stars
cutoff = 0.05
call = create_cummulative_table(dall, col_name, quantile(dataset$commits, 0:100 / 100))
cstars = create_cummulative_table(dstars, col_name, quantile(top_stars$commits, 0:100 / 100))
call$source = rep("Dataset", nrow(call))
cstars$source = rep("Top Stars", nrow(cstars))
cboth = rbind(call, cstars)
```

```{r}
```


```{r}
cutoff_all = quantile(dall[[col_name]], cutoff)
if (cutoff_all == min(dall[[col_name]])) {
  cutoff_all = cutoff_all + 1
}
cutoff_stars = quantile(dstars[[col_name]], cutoff)
column = as.symbol(col_name)
cutoff_all_pct = nrow(dall %>% filter(!!column >= cutoff_all)) / nrow(dall)
cutoff_stars_pct = nrow(dall %>% filter(!!column >= cutoff_stars)) / nrow(dall)

v_breaks = sort(c(0, 0.25, 0.5, 0.75, 1, cutoff_all_pct, cutoff_stars_pct))
v_labels = round(v_breaks * 100, 0)

h_breaks = sort(c(0, 1, 10, 100, 1000, 10000, 100000, 1000000, 10000000, 100000000, 100000000000, cutoff_all, cutoff_stars))
h_labels = sapply(h_breaks, prettify_value)

ggplot(cboth) +
  geom_line(aes(x = cutoff, y = pct, color = source)) +
  geom_vline(xintercept = cutoff_all, linetype = "dashed", color = "black") +
  geom_hline(yintercept = cutoff_all_pct, linetype = "dashed", color = "black") +
  geom_vline(xintercept = cutoff_stars, linetype = "dashed", color = "red") +
  geom_hline(yintercept = cutoff_stars_pct, linetype = "dashed", color = "red") +
  #scale_x_log10(breaks = c(0,1,10,100,1000,10000,100000), labels = c("0", "1","10", "100", "1K", "10K", "100K")) +
  scale_x_log10(breaks = h_breaks, labels = h_labels) +
  scale_y_continuous(breaks = v_breaks, labels = v_labels) + 
  scale_color_manual(values = c("black", "red")) +
  xlab(col_name) + ylab("%")




```



```{r}
```

```{r}
top_stars = dataset %>% filter(stars > 0) %>% arrange(desc(stars)) %>% head(N_TOP_STARS)
```

```{r}
create_heatmap = function(data, h_col, v_col, h_splits, v_splits) {
    data = data %>% select(c(h_col, v_col))
    heatmap_rows = length(h_splits) - 1
    heatmap_cols = length(v_splits) - 1
    remaining_and = rep(0, heatmap_rows * heatmap_cols)
    remaining_or = rep(0, heatmap_rows * heatmap_cols)
    members_and = rep(0, heatmap_rows * heatmap_cols)
    members_or = rep(0, heatmap_rows * heatmap_cols)
    x_min = rep(0, heatmap_rows * heatmap_cols)
    x_max = rep(0, heatmap_rows * heatmap_cols)
    y_min = rep(0, heatmap_rows * heatmap_cols)
    y_max = rep(0, heatmap_rows * heatmap_cols)
    row = rep(0, heatmap_rows * heatmap_cols)
    col = rep(0, heatmap_rows * heatmap_cols)
    h_col = as.symbol(h_col)
    v_col = as.symbol(v_col)
    for (v in 1:(length(v_splits) - 1)) {
        ymin = v_splits[[v]]
        ymax = v_splits[[v + 1]]
        for (h in 1:(length(h_splits) - 1)) {
          xmin = h_splits[[h]]
          xmax = h_splits[[h + 1]]
          idx = (v - 1) * heatmap_cols + h
          remaining_and[[idx]] = nrow(data %>% filter(!!h_col >=  xmin & !!v_col >= ymin))
          remaining_or[[idx]] = nrow(data %>% filter(!!h_col >=  xmin | !!v_col >= ymin))
          members_and[[idx]] = nrow(data %>% filter(!!h_col >= xmin & !!h_col < xmax & !!v_col >= ymin & !!v_col < ymax))
          members_or[[idx]] = nrow(data %>% filter((!!h_col >= xmin & !!h_col < xmax) | (!!v_col >= ymin & !!v_col < ymax)))
          x_min[[idx]] = xmin
          x_max[[idx]] = xmax
          y_min[[idx]] = ymin
          y_max[[idx]] = ymax
          row[[idx]] = v
          col[[idx]] = h
        }
    }
    data.frame(
      col, row, x_min, x_max, y_min, y_max, remaining_and, remaining_or, members_and, members_or
    )
}
```

A simple test that this works, a dataset on itself:

```{r}
heat = create_heatmap(dataset, "commits", "age", quantile(dataset$commits, c(0,10,20,30,40,50,60,70,80,90,100) / 100), quantile(dataset$age, c(0,10,20,30,40,50,60,70,80,90,100) / 100))
ggplot(heat, aes(xmin = x_min, xmax = x_max, ymin = y_min, ymax = y_max, fill = members_or / nrow(dataset))) +
  geom_rect() +
  geom_point(aes(x = x_min, y = y_min), color = "#ff0000") + 
  scale_x_log10(breaks = c(0,1,10,100,1000,10000,100000), labels = c("0", "1", "10", "100", "1K", "10K", "100K")) +
  scale_y_log10(breaks = c(0,1,10,100,1000,10000,100000,1000000, 10000000, 100000000), labels = c("0", "1", "10", "100", "1K", "10K", "100K", "1M", "10M", "100M"))  +
  scale_fill_gradient(low = "#0000ff", high = "#ffffff")

```





#heat = create_remaining_heatmap(dataset, "commits", "age", quantile(dataset$commits, 0:100 / 100), quantile(dataset$age, 0:100 / 100))
heat = create_heatmap(dataset, "commits", "age", quantile(top_stars$commits, c(0,10,20,30,40,50,60,70,80,90,100) / 100), quantile(top_stars$age, c(0,10,20,30,40,50,60,70,80,90,100) / 100))


```{r}
h_name = "commits"
v_name = "age"
h_splits = quantile(top_stars[[h_name]], seq(0, 100, by = 5) / 100)
v_splits = quantile(top_stars[[v_name]], seq(0, 100, by = 5) / 100)
heat_all = create_heatmap(dataset, h_name, v_name, h_splits, v_splits)
h_splits = quantile(dataset[[h_name]], seq(0, 100, by = 5) / 100)
v_splits = quantile(dataset[[v_name]], seq(0, 100, by = 5) / 100)
heat_stars = create_heatmap(top_stars, h_name, v_name, h_splits, v_splits)

```

```{r}
#ggplot(heat, aes(x = col, y = row, fill = remaining_or / nrow(dataset))) +
#  geom_tile()
ggplot(heat_stars, aes(xmin = x_min, xmax = x_max, ymin = y_min, ymax = y_max, fill = remaining_or / nrow(dataset))) +
  geom_rect() +
  geom_point(aes(x = x_min, y = y_min), color = "#ff0000") + 
  scale_x_log10(breaks = c(0,1,10,100,1000,10000,100000), labels = c("0", "1", "10", "100", "1K", "10K", "100K")) +
  scale_y_log10(breaks = c(0,1,10,100,1000,10000,100000,1000000, 10000000, 100000000), labels = c("0", "1", "10", "100", "1K", "10K", "100K", "1M", "10M", "100M"))  +
  scale_fill_gradient(low = "#ffffff", high = "#000000")
```

Maybe let's try different thing...

```{r}
summary(dataset$commits)
summary(dataset$age)
```

```{r}
h_splits = c(0,10,100,1000,10000,100000,1000000)
v_splits = c(0, 10,100,1000,10000,100000,1000000,10000000,100000000)
heat_all = create_heatmap(dataset, h_name, v_name, h_splits, v_splits)
heat_stars = create_heatmap(top_stars, h_name, v_name, h_splits, v_splits)
```

```{r}
ggplot(heat_all, aes(xmin = x_min, xmax = x_max, ymin = y_min, ymax = y_max, fill = members_and / nrow(dataset))) +
  geom_rect() +
  geom_point(aes(x = x_min, y = y_min), color = "#ff0000") + 
  scale_x_log10(breaks = c(0,1,10,100,1000,10000,100000), labels = c("0", "1", "10", "100", "1K", "10K", "100K")) +
  scale_y_log10(breaks = c(0,1,10,100,1000,10000,100000,1000000, 10000000, 100000000), labels = c("0", "1", "10", "100", "1K", "10K", "100K", "1M", "10M", "100M"))  +
  scale_fill_gradient(low = "#ffffff", high = "#000000")
ggplot(heat_stars, aes(xmin = x_min, xmax = x_max, ymin = y_min, ymax = y_max, fill = members_and / nrow(top_stars))) +
  geom_rect() +
  geom_point(aes(x = x_min, y = y_min), color = "#ff0000") + 
  scale_x_log10(breaks = c(0,1,10,100,1000,10000,100000), labels = c("0", "1", "10", "100", "1K", "10K", "100K")) +
  scale_y_log10(breaks = c(0,1,10,100,1000,10000,100000,1000000, 10000000, 100000000), labels = c("0", "1", "10", "100", "1K", "10K", "100K", "1M", "10M", "100M"))  +
  scale_fill_gradient(low = "#ffffff", high = "#000000")

```

# Making sense out of it

Maybe the graphs above are really hard to interpret for a reason, not just for me. An idea is to focus on a single property - the and and or additions do not add that much as they basically choose either the more or less permissive item, but the general pattern is there the same. So we are down to a simple cummulative graph 

```{r}
# creates a cummulative table for the dataset and given column
create_cummulative_table = function(data, column, breaks) {
  column = as.symbol(column)
  data = data %>% select(!!column)
  data_rows = nrow(data)
  value = rep(0, length(breaks))
  pct = rep(0, length(breaks))
  for (i in 1:length(breaks)) {
    remaining = nrow(data %>% filter(!!column >= breaks[[i]]))
    value[[i]] = remaining
    pct[[i]] = remaining / data_rows
  }
  data.frame(
    cutoff = breaks,
    members = value,
    pct = pct
  )
}
```

```{r}
dall = create_cummulative_table(dataset, "commits", quantile(dataset$commits, 0:100 / 100))
dstars = create_cummulative_table(top_stars, "commits", quantile(top_stars$commits, 0:100 / 100))
```


```{r}
cutoff = quantile(top_stars$commits, 0.05)
ggplot() +
  geom_line(data = dall, aes(x = cutoff, y = 1 - pct)) +
  geom_line(data = dstars, aes(x = cutoff, y = 1- pct), color = "#ff0000") +
  geom_vline(xintercept = cutoff, linetype = "dashed") +
  scale_x_log10(breaks = c(0,1,2,5,10,100,1000,10000,100000), labels = c("0", "1","2","5","10", "100", "1K", "10K", "100K"))

```

dataset %>% filter(commits >= quantile(dataset$commits, 0.05)) <--
dataset %>% filter(commits >= 26) <-- 



# El Stuff Ancien

This notebook presents the projects selection analysis and accompnying graphs. The idea is how to determine the cutoffs for project selections so that they are statistically meaningful, or at least not wrong. 

```{r}
java_raw = load_raw_dataset("java")
LOG("javaProjectsInDatastore", nrow(java_raw))
java_all = java_raw %>% filter(commits > 0)
LOG("javaProjectsWithCommit", nrow(java_all))
java_all = java_all %>% filter(language == "Java")
LOG("javaActualProjects", nrow(java_all))
set.seed(43)
java = sample_n(java_all, 1000000)
LOGPct("javaSampledProjects", nrow(java), nrow(java_all))
java = calculate_extra_columns(java)
```
