---
title: "Project Selection Analysis"
output: html_notebook
---

```{r}
source("artifact/helpers.R")
source("artifact/latex-log.R")
initialize_log(name = "selection")

N_TOP_STARS = 1000
```

Here is the idea: The dataset is large and we need statistically sound ways to determine how to remove the uninteresting projects from it. As an example we can say that whether projects are developed or not depends on the number of commits they have. But where to draw the line and say projects with less than N commits are not considered developed. 

The obvious simple solution is to use the widely accepted 5% cutoff. We can illustrate the bad representativeness of the top stars projects by using the 5% cutoff calculated on the top stars projects, but on the entire population. 

> Finally, instead of 5% we can use the manual analysis of top starred projects to give us different number. However we have currently found this number to be at least 2 to 5% (it can be more, but due to the way we did the manual classification, these were the numbers of projects we saw). We can either ignore this, or we can redo the manual analysis. For now this document ignores this option.

# Cummulative graphs for various attributes

First, we plot cummulative graphs for the various attributes CodeDJ offers:

```{r}
# creates a cummulative table for the dataset and given column
create_cummulative_table = function(data, column, breaks) {
  column = as.symbol(column)
  data = data %>% select(!!column)
  data_rows = nrow(data)
  value = rep(0, length(breaks))
  pct = rep(0, length(breaks))
  for (i in 1:length(breaks)) {
    remaining = nrow(data %>% filter(!!column >= breaks[[i]]))
    value[[i]] = remaining
    pct[[i]] = remaining / data_rows
  }
  data.frame(
    cutoff = breaks,
    members = value,
    pct = pct
  )
}

prettify_value = function(x) {
  if (x == 1000)
    "1K"
  else if (x == 10000)
    "10K"
  else if (x == 100000)
    "100K"
  else if (x == 1000000)
    "1M"
  else if (x == 10000000)
    "10M"
  else if (x == 100000000)
    "100M"
  else if (x == 1000000000)
    "1B"
  else if (x == 10000000000)
    "10B"
  else if (x == 100000000000)
    "100B"
  else
    paste(x)
}

# given a column name, create the cummulative graph for it
cummulative_graph = function(dall, dstars, col_name, cutoff = 0.05, title = col_name) {
  # get the cummulative tables    
  call = create_cummulative_table(dall, col_name, quantile(dall[[col_name]], 0:100 / 100))
  cstars = create_cummulative_table(dstars, col_name, quantile(dstars[[col_name]], 0:100 / 100))
  # merge them into a single data frame for easier printing
  call$source = rep("Dataset", nrow(call))
  cstars$source = rep("Top Stars", nrow(cstars))
  cboth = rbind(call, cstars)

cutoff_all = quantile(dall[[col_name]], cutoff)
  if (cutoff_all == min(dall[[col_name]])) {
    cutoff_all = cutoff_all + 1
  }
  cutoff_stars = quantile(dstars[[col_name]], cutoff)
  column = as.symbol(col_name)
  
  dcutoff = dall %>% filter(!!column >= cutoff_stars)
  
  cutoff_all_pct = nrow(dall %>% filter(!!column >= cutoff_all)) / nrow(dall)
  cutoff_stars_pct = nrow(dcutoff) / nrow(dall)
  
  ccutoff = create_cummulative_table(dcutoff, col_name, quantile(dcutoff[[col_name]], 0:100 / 100))
  ccutoff$source = "Remainder"
  cboth = rbind(cboth, ccutoff)
  
  v_breaks = sort(c(0, 0.25, 0.5, 0.75, 1, cutoff_all_pct, cutoff_stars_pct))
  v_labels = round(v_breaks * 100, 0)
  
  h_breaks = sort(c(0, 1, 10, 100, 1000, 10000, 100000, 1000000, 10000000, 100000000, 100000000000, cutoff_all, cutoff_stars))
  h_labels = sapply(h_breaks, prettify_value)
  
  plot = ggplot(cboth) +
    geom_line(aes(x = cutoff, y = pct, color = source)) +
    geom_vline(xintercept = cutoff_all, linetype = "dashed", color = "black") +
    geom_hline(yintercept = cutoff_all_pct, linetype = "dashed", color = "black") +
    geom_vline(xintercept = cutoff_stars, linetype = "dashed", color = "red") +
    geom_hline(yintercept = cutoff_stars_pct, linetype = "dashed", color = "red") +
    #scale_x_log10(breaks = c(0,1,10,100,1000,10000,100000), labels = c("0", "1","10", "100", "1K", "10K", "100K")) +
    scale_x_log10(breaks = h_breaks, labels = h_labels) +
    scale_y_continuous(breaks = v_breaks, labels = v_labels) + 
    scale_color_manual(values = c("black", "gray", "red")) +
    xlab(title) + ylab("%") +
    theme_classic()
  list(
  name = col_name, 
    title = title,
    plot = plot, cutoff_all = cutoff_all, cutoff_stars = cutoff_stars
  )
}
```

Let's do the graphs for the attributes we are looking into now for all three languages:

```{r}
columns = c("max_hindex1", "lifetime", "users", "locs", "snapshots", "commits")
names(columns) = c("C-Index", "Age", "Devs", "Locs", "Versions", "Commits")

language_plots = function(dataset, top_stars, columns) {
  cdata = list()
  for (name in names(columns)) {
    cdata[[name]] = cummulative_graph(dataset, top_stars, columns[[name]], title = name)
  }
  cdata  
}
```

```{r}
# java
java_dataset = java
java_topstars = java_dataset %>% filter(stars > 0) %>% arrange(desc(stars)) %>% head(N_TOP_STARS)
cjava = language_plots(java_dataset, java_topstars, columns)
#python
python_dataset = python
python_topstars = python_dataset %>% filter(stars > 0) %>% arrange(desc(stars)) %>% head(N_TOP_STARS)
cpython = language_plots(python_dataset, python_topstars, columns)
#js
js_dataset = js
js_topstars = js_dataset %>% filter(stars > 0) %>% arrange(desc(stars)) %>% head(N_TOP_STARS)
cjs = language_plots(js_dataset, js_topstars, columns)
```
### Java

```{r}
cjava[["C-Index"]]$plot
cjava[["Age"]]$plot
cjava[["Devs"]]$plot
cjava[["Locs"]]$plot
cjava[["Versions"]]$plot
cjava[["Commits"]]$plot
```

### Python

```{r}
cpython[["C-Index"]]$plot
cpython[["Age"]]$plot
cpython[["Devs"]]$plot
cpython[["Locs"]]$plot
cpython[["Versions"]]$plot
cpython[["Commits"]]$plot
```

### JavaScript

```{r}
cjs[["C-Index"]]$plot
cjs[["Age"]]$plot
cjs[["Devs"]]$plot
cjs[["Locs"]]$plot
cjs[["Versions"]]$plot
cjs[["Commits"]]$plot
```

# Datasets for analysis

For each language, we prepare the following three datasets:

- the top 1k stars, without the projects below the cutoff (i.e. ~5% of the top stars) (the red line after the *red* cutoff above)
- the full dataset without projects below the top stars 5% cutoff (the black line after the *red* cutoff above)
- the full dataset without lowest 5% projects (the black line after the black cutoff above)

These should roughly correspond to the top stars w/o misfits, the definitely interesting projects and the definitely uninteresting projects removed. 

```{r}
create_datasets = function(dataset, top_stars, cutoffs) {
  stars = top_stars
  good = dataset
  not_bad = dataset
  for (x in cutoffs) {
    colname = as.symbol(x$name)
    stars = stars %>% filter(!!colname >= x$cutoff_stars)
    good = good %>% filter(!!colname >= x$cutoff_stars)
    not_bad = not_bad %>% filter(!!colname >= x$cutoff_all)
  }
  cat(paste("top_stars:", nrow(stars), " (", nrow(stars) * 100 / nrow(top_stars), "%)\n"))
  cat(paste("good:", nrow(good), " (", nrow(good) * 100 / nrow(dataset), "%)\n"))
  cat(paste("not_bad:", nrow(good), " (", nrow(not_bad) * 100 / nrow(dataset), "%)\n"))
  list(stars = stars, good = good, not_bad = not_bad)
}
```

```{r}
cat("Java:\n");
java_datasets = create_datasets(java_dataset, java_topstars, cjava)
cat("Python:\n");
python_datasets = create_datasets(python_dataset, python_topstars, cjava)
cat("JS:\n");
js_datasets = create_datasets(js_dataset, js_topstars, cjava)
```

